{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_and_plaintext_recovery.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WarUaT9fodt3",
        "colab_type": "text"
      },
      "source": [
        "# Model and Plaintext Recovery\n",
        "The first segment of this notebook involves the definition and training of the character level language model. The second segment contains the implementation of the plaintext recovery of the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVVrK60Pwd5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U skorch\n",
        "import skorch\n",
        "!pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMKQl2gHAxT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "import tarfile\n",
        "import io\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import string\n",
        "import unicodedata\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from skorch import helper, callbacks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1keG8xMOC64d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.mount('/content/gdrive')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOUQ75sUeMma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp '/content/gdrive/My Drive/smaller_subset-cleaned.zip' 'smaller_subset-cleaned.zip'\n",
        "!ls -al"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlmO048xfGH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip smaller_subset-cleaned.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdbMcyGZpFaP",
        "colab_type": "text"
      },
      "source": [
        "# Character Level LSTM Language Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-VtAiZ1HU4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define Model\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, num_of_layers, seq_len, dropout, padding_idx=0):\n",
        "        super(Model, self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_of_layers = num_of_layers\n",
        "        self.embedding = nn.Embedding(input_size, input_size, padding_idx=padding_idx)\n",
        "        self.input_dropout = nn.Dropout(dropout, inplace=False)\n",
        "        # Hidden LSTM layer\n",
        "        self.lstm = nn.LSTM(input_size, hidden_dim, num_of_layers, batch_first = True)\n",
        "        # Fully connected output layer\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_size)\n",
        "        self.norm = nn.LayerNorm(output_size)\n",
        "\n",
        "    def forward(self, inp, hidden, seq_lengths):\n",
        "        inp = self.embedding(inp)\n",
        "        inp = self.input_dropout(inp)\n",
        "        max_seq_length = max(seq_lengths)\n",
        "        inp = nn.utils.rnn.pack_padded_sequence(inp, seq_lengths, enforce_sorted=False, batch_first=True) # pad sequences\n",
        "        packed_output, new_hidden = self.lstm(inp, hidden)\n",
        "        output, inp_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, padding_value=0, batch_first=True, total_length=self.seq_len) # unpack sequences back to tensors\n",
        "        output = output.to(device)\n",
        "        output_fc = self.fc2(output) # output with correct dimensions\n",
        "        output_norm = self.norm(output_fc)\n",
        "        return (output_norm, new_hidden)\n",
        "    \n",
        "    def initState(self, batch_size):\n",
        "        return (torch.zeros(self.num_of_layers, batch_size, self.hidden_dim),\n",
        "                torch.zeros(self.num_of_layers, batch_size, self.hidden_dim))\n",
        "\n",
        "class LSTMNet(skorch.net.NeuralNet):\n",
        "    def on_epoch_begin(self, *args, **kwargs):\n",
        "        super().on_epoch_begin(*args, **kwargs)\n",
        "      \n",
        "        self.hidden = self.module_.initState(self.batch_size)\n",
        "\n",
        "    def train_step(self, X, y):\n",
        "        self.module_.train()\n",
        "        self.optimizer_.zero_grad()\n",
        "        inp = torch.stack(X[0]).transpose(0,1)\n",
        "        inp_seq_len = X[1]\n",
        "        target = torch.stack(y[0]).transpose(0,1)\n",
        "        target_seq_len = y[1]\n",
        "        inp = inp.to(device)\n",
        "        target = nn.utils.rnn.pad_sequence(target, batch_first=True)\n",
        "        target = target.to(device)\n",
        "        \n",
        "\n",
        "        hidden = tuple([each.data.to(device) for each in self.hidden])\n",
        "\n",
        "        output, self.hidden = self.module_(inp, hidden, inp_seq_len)\n",
        "        criterion_input = output.transpose(1,2)\n",
        "        criterion_input.to(device)\n",
        "        loss = self.get_loss(criterion_input, target)\n",
        "        loss.to(device)\n",
        "        loss.backward(retain_graph=True)\n",
        "        self.optimizer_.step()\n",
        "\n",
        "        return { 'loss': loss, 'y_pred': output }\n",
        "    \n",
        "    def validation_step(self, X, y):\n",
        "        self.module_.eval()\n",
        "        with torch.no_grad():\n",
        "            inp = torch.stack(X[0]).transpose(0,1)\n",
        "            inp_seq_len = X[1]\n",
        "            target = torch.stack(y[0]).transpose(0,1)\n",
        "            target_seq_len = y[1]\n",
        "            inp = inp.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            hidden = self.module_.initState(self.batch_size)\n",
        "            hidden = tuple([each.data.to(device) for each in hidden])\n",
        "            output, _ = self.module_(inp, hidden, inp_seq_len)\n",
        "            criterion_input = output.transpose(1, 2)\n",
        "            target = nn.utils.rnn.pad_sequence(target, batch_first=True)\n",
        "            loss = self.get_loss(criterion_input, target)\n",
        "            \n",
        "            return {'loss': loss, 'y_pred': output }\n",
        "        \n",
        "    def evaluation_step(self, X, **kwargs):\n",
        "        self.module_.eval()\n",
        "        with torch.no_grad():\n",
        "            inp = torch.stack(X[0]).transpose(0,1)\n",
        "            inp_seq_len = X[1]\n",
        "            inp = inp.to(device)\n",
        "\n",
        "            hidden = self.module_.initState(self.batch_size)\n",
        "            hidden = tuple([each.data.to(device) for each in hidden])\n",
        "            output, _ = self.module_(inp, hidden, inp_seq_len)\n",
        "            return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLukqsBPD1Pe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# num_of_letters = len(letters) + 3\n",
        "# padding index is 0\n",
        "separator = '{sep}'\n",
        "start_of_file = '{start}' # index is num_of_letters - 2\n",
        "end_of_file = '{end}' # index is num_of_letters - 1\n",
        "\n",
        "# read data\n",
        "def readFromFile(input_filename, output_filename, max_length):\n",
        "    # total length of line will be max_length + 1 (last letter is for creation of target tensor)\n",
        "    try:\n",
        "        text = open(input_filename, encoding='ascii').read().strip()\n",
        "    except:\n",
        "        print(\"Could not open in ascii: \" + input_filename)\n",
        "        return 0, ''\n",
        "    with open(output_filename, 'a') as output:\n",
        "        line = start_of_file + text[0: max_length] + separator\n",
        "        letters = ''.join(set(line))\n",
        "        counter = 1\n",
        "        output.write(line)\n",
        "        for i in range(max_length - 1, len(text), max_length):\n",
        "            line = text[i: i + max_length + 1]\n",
        "            letters = ''.join(set(letters + line))\n",
        "            if i >= len(text) - max_length:\n",
        "                line += end_of_file\n",
        "            line += separator\n",
        "            counter += 1\n",
        "            output.write(line)\n",
        "        output.close()\n",
        "    return counter, letters\n",
        "\n",
        "def writeFilenameToFile(test_file, filename):\n",
        "    try:\n",
        "        text = open(filename, encoding='ascii').read().strip() # ensure it is readable in ascii\n",
        "    except:\n",
        "        return\n",
        "    with open(test_file, 'a') as output:\n",
        "        output.write(filename + \"\\n\")\n",
        "    output.close()\n",
        "\n",
        "def readLinesFromData(directory, output_filename, max_length, test_file, for_every):\n",
        "    files = readFilesFromData(directory)\n",
        "    letters = ''\n",
        "    num_of_lines = 0\n",
        "    random.shuffle(files) # shuffle files\n",
        "    for i in range(len(files)):\n",
        "        filename = files[i]\n",
        "        if i % for_every == 0:\n",
        "            writeFilenameToFile(test_file, filename)\n",
        "        else:\n",
        "            num_from_file, letters_from_file = readFromFile(filename, output_filename, max_length)\n",
        "            letters = ''.join(set(letters + letters_from_file))\n",
        "            num_of_lines += num_from_file\n",
        "    return num_of_lines, letters\n",
        "\n",
        "def readFilesFromData(directory_path):\n",
        "    all_files = []\n",
        "    for root, directories, filenames in os.walk(directory_path):\n",
        "        for directory in directories:\n",
        "            all_files.extend(readFilesFromData(directory))\n",
        "        for filename in filenames:\n",
        "            all_files.append(os.path.join(root, filename))\n",
        "    return all_files\n",
        "\n",
        "\n",
        "class CharacterDataset(Dataset):\n",
        "    def __init__(self, input_dir, max_length, for_every=0):\n",
        "        self.input_dir = input_dir\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.processed_file = os.path.join(os.getcwd(), 'enron_processed.txt')\n",
        "        self.test_file = os.path.join(os.getcwd(), 'enron_test_files.txt')\n",
        "        \n",
        "        if os.path.exists(self.processed_file):\n",
        "            os.remove(self.processed_file)    \n",
        "        if os.path.exists(self.test_file):\n",
        "            os.remove(self.test_file)\n",
        "        \n",
        "        self.num_of_lines, self.letters = readLinesFromData(input_dir, self.processed_file, max_length, self.test_file, for_every)\n",
        "        print(\"number of lines: \" + str(self.num_of_lines))\n",
        "        self.num_of_letters = len(self.letters) + 3 # plus three for padding character, start_of_file marker and end_of_file marker\n",
        "        self.split_text = open(self.processed_file, encoding='ascii').read().split(separator)\n",
        "\n",
        "    def getLetterIndices(self, line):\n",
        "        letter_indices = []\n",
        "        start_index = line.find(start_of_file)\n",
        "        end_index = line.find(end_of_file)\n",
        "        letter_index = 0\n",
        "        while letter_index < len(line):\n",
        "            if letter_index == start_index:\n",
        "                letter_indices.append(self.num_of_letters - 2)\n",
        "                letter_index += len(start_of_file)\n",
        "            elif letter_index == end_index:\n",
        "                letter_indices.append(self.num_of_letters - 1)\n",
        "                letter_index += len(end_of_file)\n",
        "            else:\n",
        "                letter = line[letter_index]\n",
        "                letter_indices.append(self.letters.find(letter) + 1) #shifted by 1 because of padding marker\n",
        "                letter_index += 1\n",
        "        original_length = len(letter_indices)\n",
        "        return letter_indices, original_length\n",
        "\n",
        "    # Input tensor contains indices of letters excluding the last letter in the line\n",
        "    def createInputTensor(self, line, max_length):\n",
        "        letter_indices, length = self.getLetterIndices(line)\n",
        "        inp_indices = letter_indices[:-1]\n",
        "        inp_length = length - 1\n",
        "        while len(inp_indices) < max_length:\n",
        "            inp_indices.append(0)\n",
        "        return (inp_indices, inp_length)\n",
        "\n",
        "    # Target tensor contains indices of letters in the input tensor \n",
        "    # excluding the first letter\n",
        "    def createTargetTensor(self, line, max_length):\n",
        "        letter_indices, length = self.getLetterIndices(line)\n",
        "        target_indices = letter_indices[1:]\n",
        "        target_length = length - 1\n",
        "        while len(target_indices) < max_length:\n",
        "            target_indices.append(0)\n",
        "        return (target_indices, target_length)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.split_text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        line = self.split_text[index]\n",
        "        input = self.createInputTensor(line, self.max_length)\n",
        "        target = self.createTargetTensor(line, self.max_length)\n",
        "        return input, target\n",
        "    \n",
        "    def getLetters(self):\n",
        "        return self.letters, self.num_of_letters\n",
        "\n",
        "\n",
        "def getMatchesAndTotal(output, target_seq):\n",
        "    prob = nn.functional.softmax(torch.from_numpy(output), dim=2) \n",
        "    char_indices = torch.max(prob, dim=2)[1]\n",
        "    target_seq = torch.tensor(target_seq)\n",
        "    target_seq = target_seq[:char_indices.size(0)]\n",
        "    matches = torch.eq(char_indices, target_seq).sum().item()\n",
        "    total = torch.numel(char_indices)\n",
        "    return matches, total\n",
        "\n",
        "def ds_accuracy(net, ds, y=None):\n",
        "    y_true = [y[0] for _, y in ds]\n",
        "    y_pred = net.predict(ds)\n",
        "    matches, total = getMatchesAndTotal(y_pred, y_true)\n",
        "    return (matches / total) * 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVIZ1ISVKb6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_len = 150\n",
        "is_cuda = torch.cuda.is_available()\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")\n",
        "\n",
        "def initState(batch_size, num_of_layers, hidden_dim):\n",
        "    return (torch.zeros(num_of_layers, batch_size, hidden_dim),\n",
        "            torch.zeros(num_of_layers, batch_size, hidden_dim))\n",
        "dataset = CharacterDataset('smaller_subset', seq_len, for_every=30)\n",
        "letters, num_of_letters = dataset.getLetters()\n",
        "\n",
        "print(\"letters: \" + letters)\n",
        "\n",
        "batch_size = 256\n",
        "num_of_epochs = 50\n",
        "learning_rate = 0.001\n",
        "hidden_dim = int((2/3) * num_of_letters + num_of_letters)\n",
        "num_of_layers = 2\n",
        "dropout=0.2\n",
        "init_state = initState(batch_size, num_of_layers, hidden_dim)\n",
        "net = LSTMNet(\n",
        "    module=Model,\n",
        "    module__input_size=num_of_letters,\n",
        "    module__output_size=num_of_letters,\n",
        "    module__hidden_dim = hidden_dim,\n",
        "    module__num_of_layers=num_of_layers,\n",
        "    module__seq_len=seq_len,\n",
        "    module__dropout=dropout,\n",
        "    module__padding_idx=0,\n",
        "    device=device,\n",
        "    criterion=torch.nn.CrossEntropyLoss,\n",
        "    criterion__ignore_index=0,\n",
        "    optimizer=torch.optim.Adam,\n",
        "    optimizer__lr=learning_rate,\n",
        "    batch_size=batch_size,\n",
        "    max_epochs=num_of_epochs,\n",
        "    train_split=skorch.dataset.CVSplit(10),\n",
        "    callbacks=[callbacks.EpochScoring(ds_accuracy, use_caching=False), \n",
        "        callbacks.EarlyStopping(patience=30, threshold_mode='abs'), \n",
        "        callbacks.Checkpoint(dirname='checkpoints'),\n",
        "        callbacks.TrainEndCheckpoint(dirname='checkpoints'),\n",
        "        callbacks.LoadInitState(callbacks.Checkpoint(dirname='checkpoints'))],\n",
        "    iterator_train__drop_last=True,\n",
        "    iterator_valid__drop_last=True\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVll3EDEkzBZ",
        "colab_type": "text"
      },
      "source": [
        "Model Training is done in the next three cells.\n",
        "\n",
        "Do not run the following three cells if just doing prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLIa1hXaKeYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    with open(\"letters.txt\", 'w') as letters_file:\n",
        "        letters_file.write(letters)\n",
        "\n",
        "    print(\"number_of_letters: \" + str(num_of_letters))\n",
        " \n",
        "    print(\"Finished preprocessing....\")\n",
        "\n",
        "    # hyperparameters\n",
        "    with torch.autograd.set_detect_anomaly(True):\n",
        "        net.set_params(device=device)\n",
        "        net.fit(dataset)\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYzHT8rXGrdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!zip -r checkpoints.zip checkpoints/\n",
        "from google.colab import files\n",
        "files.download(\"checkpoints.zip\")\n",
        "files.download(\"enron_processed.txt\")\n",
        "files.download(\"enron_test_files.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnJDRX7XZklJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download(\"letters.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT5IwAhTknFm",
        "colab_type": "text"
      },
      "source": [
        "# Plaintext recovery\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXxP6RYWfwjK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upload checkpoints\n",
        "!cp '/content/gdrive/My Drive/checkpoints.zip' 'checkpoints.zip'\n",
        "!cp '/content/gdrive/My Drive/enron_processed.txt' 'enron_processed.txt'\n",
        "!cp '/content/gdrive/My Drive/enron_test_files.txt' 'enron_test_files.txt'\n",
        "!cp '/content/gdrive/My Drive/letters.txt' 'letters.txt'\n",
        "!ls -al\n",
        "!unzip checkpoints.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AY_8QinGjsk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.nn import Softmax\n",
        "import os\n",
        "import skorch\n",
        "from skorch import callbacks\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from skorch.callbacks import Checkpoint\n",
        "import string\n",
        "import numpy as np\n",
        "from skorch.helper import SliceDataset\n",
        "import math\n",
        "import time\n",
        "from google.colab import files\n",
        "\n",
        "letters = open('letters.txt', 'r').read()\n",
        "num_of_letters = len(letters) + 3\n",
        "cp = Checkpoint(dirname='checkpoints', fn_prefix='train_end_')\n",
        "net.initialize()\n",
        "net.load_params(checkpoint=cp)\n",
        "model1, model2 = net, net\n",
        "\n",
        "def getLongerString(string1, string2):\n",
        "    if len(string1) > len(string2):\n",
        "        return string1\n",
        "    else:\n",
        "        return string2\n",
        "\n",
        "def xor_string(string1, string2):\n",
        "    xored = []\n",
        "    longer_string = getLongerString(string1, string2)\n",
        "    for i in range(min(len(string1), len(string2))):\n",
        "        xored_value = chr(ord(string1[i]) ^ ord(string2[i]))\n",
        "        xored.append(xored_value)\n",
        "    for j in range(min(len(string1), len(string2)), len(longer_string)):\n",
        "        xored.append(longer_string[j])\n",
        "    return ''.join(xored).encode('ascii')\n",
        "\n",
        "# Ciphers will be generated up to the minimum of the length of the shorter file and the variable length (if provided)\n",
        "def createCipherTexts(test_files_path, length=None):\n",
        "    ciphers = [] # (original_plaintext1, original_plaintext2, cipher)\n",
        "    with open(test_files_path, 'r') as f:\n",
        "        files = f.readlines()\n",
        "        f.close()\n",
        "        for i in range(0, len(files), 2):\n",
        "            if i + 1 >= len(files):\n",
        "                # take note of outstanding files\n",
        "                with open(\"outstanding.txt\", 'a+') as outstanding_file:\n",
        "                    outstanding_file.write(files[i])\n",
        "                break\n",
        "            first_filename = files[i][:-1]\n",
        "            second_filename = files[i + 1][:-1]\n",
        "            first_plaintext = open(first_filename, encoding='ascii').read().strip()\n",
        "            second_plaintext = open(second_filename, encoding='ascii').read().strip()\n",
        "            length_of_shorter_string = min(len(first_plaintext), len(second_plaintext))\n",
        "            if length is None:\n",
        "                length = length_of_shorter_string\n",
        "            first_plaintext = first_plaintext[:min(length, length_of_shorter_string)]\n",
        "            second_plaintext = second_plaintext[:min(length, length_of_shorter_string)]\n",
        "            cipher = xor_string(first_plaintext, second_plaintext)\n",
        "            ciphers.append((first_plaintext, second_plaintext, cipher))\n",
        "    return ciphers\n",
        "\n",
        "ciphers = createCipherTexts(\"enron_test_files.txt\", 1000)\n",
        "\n",
        "class SampleDataset(Dataset):\n",
        "    def __init__(self, input_characters, max_length, letters, num_of_letters):\n",
        "        self.input_characters = input_characters\n",
        "        self.max_length = max_length\n",
        "        self.letters = letters\n",
        "        self.num_of_letters = num_of_letters\n",
        "        self.lines = self.getLines()\n",
        "\n",
        "    def __len__(self):\n",
        "        if len(self.lines) == 0:\n",
        "            return 1\n",
        "        else:\n",
        "            return len(self.lines)\n",
        "\n",
        "    def getLines(self):\n",
        "        indices = self.getLetterIndices(self.input_characters)\n",
        "        lines = [indices[i: i + self.max_length] for i in range(0, len(indices), self.max_length)]\n",
        "        return lines\n",
        "\n",
        "    def getLetterIndices(self, line):\n",
        "        letter_indices = []\n",
        "        start_index = line.find(start_of_file)\n",
        "        end_index = line.find(end_of_file)\n",
        "        letter_index = 0\n",
        "        while letter_index < len(line):\n",
        "            if letter_index == start_index:\n",
        "                letter_indices.append(self.num_of_letters - 2)\n",
        "                letter_index += len(start_of_file)\n",
        "            elif letter_index == end_index:\n",
        "                letter_indices.append(self.num_of_letters - 1)\n",
        "                letter_index += len(end_of_file)\n",
        "            else:\n",
        "                letter = line[letter_index]\n",
        "                letter_indices.append(self.letters.find(letter) + 1) #shifted by 1 because of padding marker\n",
        "                letter_index += 1\n",
        "        return letter_indices\n",
        "\n",
        "    # Input tensor contains indices of letters excluding the last letter in the line\n",
        "    def createInputTensor(self, idx, max_length):\n",
        "        inp_indices = self.lines[idx]\n",
        "        inp_length = len(inp_indices)\n",
        "        while len(inp_indices) < max_length:\n",
        "            inp_indices.append(0)\n",
        "        return (inp_indices, inp_length)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.createInputTensor(idx, self.max_length), self.createInputTensor(idx, self.max_length)\n",
        "    \n",
        "    def numOfLines(self):\n",
        "        if len(self.lines) == 0:\n",
        "            return 1\n",
        "        else:\n",
        "            return len(self.lines)\n",
        "\n",
        "def convert_to_hex(character):\n",
        "    return character.encode('ascii')\n",
        "\n",
        "softmax = nn.Softmax(dim=2)\n",
        "prev_batch_size = 1\n",
        "net.set_params(batch_size=prev_batch_size)\n",
        "# If length of input_characters increase\n",
        "# beyond sequence length, batch size is changed to speed up prediction\n",
        "def getProbabilities(input_characters, model):\n",
        "    sample_dataset = SampleDataset(input_characters, seq_len, letters, num_of_letters)\n",
        "    num_of_lines = sample_dataset.numOfLines()\n",
        "    global prev_batch_size\n",
        "    if num_of_lines != prev_batch_size:\n",
        "        prev_batch_size = num_of_lines\n",
        "        net.set_params(batch_size=prev_batch_size)\n",
        "    output = net.predict(sample_dataset)\n",
        "    output = softmax(torch.from_numpy(output))\n",
        "    character_index = (len(input_characters) - 1) % 150\n",
        "    if input_characters.find(start_of_file) >= 0:\n",
        "        character_index -= (len(start_of_file) - 1) \n",
        "    if input_characters.find(end_of_file) >= 0:\n",
        "        character_index -= (len(end_of_file) - 1)\n",
        "    char_indices = torch.max(output, dim=2)[1][-1]\n",
        "    probability_next_char = output[-1][character_index]\n",
        "    return probability_next_char\n",
        "\n",
        "\n",
        "cipher_to_pairs_dict = {}\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")\n",
        "printable_chars = bytes(string.printable, 'ascii')\n",
        "\n",
        "\n",
        "# this function returns all possible pairs of characters that when xored,\n",
        "# will return cipher_hex\n",
        "def getPossiblePairs(cipher_hex):\n",
        "    cipher_hex = cipher_hex.decode('ascii')\n",
        "    if (cipher_hex in cipher_to_pairs_dict.keys()):\n",
        "        return cipher_to_pairs_dict[cipher_hex]\n",
        "    else:\n",
        "        pairs = []\n",
        "        visited_letters = set()\n",
        "        for i in range(len(letters)):\n",
        "            letter = letters[i]\n",
        "            xor_byte = xor_string(letter, cipher_hex)\n",
        "            if xor_byte not in printable_chars:\n",
        "                continue\n",
        "            xor = xor_byte.decode('ascii')\n",
        "            if xor in visited_letters or xor == letter:\n",
        "                if xor != letter:\n",
        "                    pairs.append((xor, letter))\n",
        "                pairs.append((letter, xor))\n",
        "            visited_letters.add(letter)\n",
        "        cipher_to_pairs_dict[cipher_hex] = pairs\n",
        "    return pairs\n",
        "\n",
        "def getProbabilitiesOfNextCharacters(current_string_pairs, model1, model2):\n",
        "    probabilities = []\n",
        "    for pair in current_string_pairs:\n",
        "        first_string_probability = getProbabilities(pair[0], model1)\n",
        "        second_string_probability = getProbabilities(pair[1], model2)\n",
        "        probabilities.append((first_string_probability, second_string_probability))\n",
        "    return probabilities\n",
        "\n",
        "def getCost(path, current_cost, probability_pair, char1_index, char2_index):\n",
        "    prob = probability_pair[0][char1_index] * probability_pair[1][char2_index]\n",
        "    negative_log = -math.log(prob)\n",
        "    total_cost = current_cost + negative_log\n",
        "    return total_cost\n",
        "\n",
        "def getLowestCostPath(current_paths, probabilities_of_next_letters, char1, char2):\n",
        "    char1_index, char2_index = letters.find(char1) + 1, letters.find(char2) + 1 # shifted by one for padding marker\n",
        "    first_path = current_paths[0]\n",
        "    cost_of_first_path = first_path[1]\n",
        "    probability_pair = probabilities_of_next_letters[0]\n",
        "    # arbitrarily set to first path\n",
        "    minimum = getCost(first_path, cost_of_first_path, probability_pair, char1_index, char2_index) \n",
        "    cheapest_path = current_paths[0]\n",
        "    for index in range(1, len(current_paths)):\n",
        "        path = current_paths[index]\n",
        "        cost = path[1]\n",
        "        probability_pair = probabilities_of_next_letters[index]\n",
        "        total_cost = getCost(path, cost, probability_pair, char1_index, char2_index)\n",
        "        if total_cost < minimum:\n",
        "            cheapest_path = path\n",
        "            minimum = total_cost\n",
        "    return createNewPath(cheapest_path, char1, char2, minimum)\n",
        "            \n",
        "def createNewPath(current_path, char1, char2, new_cost):\n",
        "    new_string1 = current_path[0][0] + char1\n",
        "    new_string2 = current_path[0][1] + char2\n",
        "    return ((new_string1, new_string2), new_cost)\n",
        "\n",
        "def getPathsForNextStep(possible_pairs, current_paths, probabilities_of_next_letters, best_k):\n",
        "    next_step_paths = []\n",
        "    for pair in possible_pairs:\n",
        "        char1, char2 = pair[0], pair[1]\n",
        "        new_path = getLowestCostPath(current_paths, probabilities_of_next_letters, char1, char2)\n",
        "        next_step_paths.append(new_path)\n",
        "    next_step_paths = sorted(next_step_paths, key=lambda path: path[1])[:best_k]\n",
        "    return next_step_paths\n",
        "\n",
        "# assume ciphertext is in hexadecimal characters\n",
        "def getPlainText(ciphertext, model1, model2, best_k):\n",
        "    first_start = start_of_file\n",
        "    second_start = start_of_file\n",
        "    current_paths = [((first_start, second_start), 0)] # [((string1, string2), negative log cost]\n",
        "    print(\"length of cipher text: \" + str(len(ciphertext)))\n",
        "    for hex_character_index in range(len(ciphertext)):\n",
        "        start = time.time()\n",
        "        print(\"currently at character: \" + str(hex_character_index))\n",
        "        character_hex = ciphertext[hex_character_index: hex_character_index + 1]\n",
        "        current_string_pairs = list(map(lambda path: path[0], current_paths))\n",
        "        probabilities_of_next_letters = getProbabilitiesOfNextCharacters(current_string_pairs, model1, model2)\n",
        "        possible_pairs = getPossiblePairs(character_hex)\n",
        "        current_paths = getPathsForNextStep(possible_pairs, current_paths, probabilities_of_next_letters, best_k)\n",
        "        end = time.time()\n",
        "        time_taken = end - start\n",
        "        print(\"time taken: \" + str(time_taken))\n",
        "    return current_paths[0][0][0], current_paths[0][0][1]  \n",
        "\n",
        "def calculateAccuracy(first_plaintext, second_plaintext, first_original, second_original):\n",
        "    num_of_matches = 0\n",
        "    for i in range(len(first_plaintext)):\n",
        "        recovered_first = first_plaintext[i]\n",
        "        recovered_second = second_plaintext[i]\n",
        "        original_first = first_original[i]\n",
        "        original_second = second_original[i]\n",
        "        if recovered_first == original_first and recovered_second == original_second:\n",
        "            num_of_matches += 2\n",
        "        elif recovered_first == original_second and recovered_second == original_first: # consider switching streams as accurately recovered\n",
        "            num_of_matches += 2\n",
        "        elif recovered_first == original_first or recovered_first == original_second:\n",
        "            num_of_matches += 1\n",
        "        elif recovered_second == original_first or recovered_second == original_second:\n",
        "            num_of_matches += 1\n",
        "        else:\n",
        "            continue\n",
        "    return (num_of_matches) / (len(first_original) + len(second_original)) * 100\n",
        "\n",
        "best_k = 50 # prune to best 50 at each time step\n",
        "for cipher in ciphers:\n",
        "    with open(\"results.txt\", 'a+') as f:\n",
        "        first_original = cipher[0]\n",
        "        second_original = cipher[1]\n",
        "        ciphertext = cipher[2]\n",
        "        first_plaintext, second_plaintext = getPlainText(ciphertext, model1, model2, best_k)\n",
        "        first_plaintext = first_plaintext.replace(start_of_file, '')\n",
        "        second_plaintext = second_plaintext.replace(start_of_file, '')\n",
        "        print(first_plaintext)\n",
        "        print(second_plaintext)\n",
        "        f.write(\"FIRST ORIGINAL:\\n\" + first_original + \"\\n\")\n",
        "        f.write(\"FIRST RECOVERED:\\n\" + first_plaintext + \"\\n\")\n",
        "        f.write(\"SECOND ORIGINAL:\\n\" + second_original + \"\\n\")\n",
        "        f.write(\"SECOND RECOVERED:\\n\" + second_plaintext + \"\\n\")\n",
        "        f.write(\"ACCURACY:\\n\" + str(calculateAccuracy(first_plaintext, second_plaintext, first_original, second_original)) + \"\\n\")\n",
        "    f.close()\n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiMAsHOgJkvZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download(\"/content/results.txt\")\n",
        "files.download(\"/content/outstanding.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}